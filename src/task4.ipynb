{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# (Un)supervised node prediction\n",
    "\n",
    "##  Random Walk based embeddings\n",
    "\n",
    "Use data from https://github.com/shchur/gnn-benchmark/tree/master/data/npz , particulary use Amazon Computers dataset, see description in a corresponding paper https://arxiv.org/pdf/1811.05868.pdf. \n",
    "> Recall that this network has small amount of isolated nodes.\n",
    "\n",
    "1. Run `DeepWalk` to get embeddings of size 32.\n",
    "2. Using `kmeans` with appropriate number of clusters (somewhere between 6 and 12) compute node labels. Compare the result with ground truth communities using `adjusted_rand_index`. Compute corresponding `Modularities`.\n",
    "3. Run your favourite dimensionality reduction algorithm to get a 2 dimensional embedding.\n",
    "4. Compare results (repeate 1-3) with embeddings of size 64 and 128.\n",
    "5. Compare results with supervised Logistic Regression on data feature matrix (without using network data).\n",
    "\n",
    "\n",
    "Recommended for visualization:\n",
    "\n",
    "- Dmitry Ulyanov has nice multicore tsne implementation https://github.com/DmitryUlyanov/Multicore-TSNE\n",
    "- Recent paper from Aleksandr Artemenkov and Maxim Panov https://arxiv.org/pdf/2001.11411.pdf with implementation https://github.com/alartum/ncvis . Reported to be superior to TSNE for the purpose of 2 dimensional visualization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import networkx as nx\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.metrics import adjusted_rand_score\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "usage: deepwalk [-h] [--debug] [--format FORMAT] --input [INPUT] [-l LOG]\n",
      "                [--matfile-variable-name MATFILE_VARIABLE_NAME]\n",
      "                [--max-memory-data-size MAX_MEMORY_DATA_SIZE]\n",
      "                [--number-walks NUMBER_WALKS] --output OUTPUT\n",
      "                [--representation-size REPRESENTATION_SIZE] [--seed SEED]\n",
      "                [--undirected UNDIRECTED] [--vertex-freq-degree]\n",
      "                [--walk-length WALK_LENGTH] [--window-size WINDOW_SIZE]\n",
      "                [--workers WORKERS]\n",
      "\n",
      "options:\n",
      "  -h, --help            show this help message and exit\n",
      "  --debug               drop a debugger if an exception is raised. (default:\n",
      "                        False)\n",
      "  --format FORMAT       File format of input file (default: adjlist)\n",
      "  --input [INPUT]       Input graph file (default: None)\n",
      "  -l LOG, --log LOG     log verbosity level (default: INFO)\n",
      "  --matfile-variable-name MATFILE_VARIABLE_NAME\n",
      "                        variable name of adjacency matrix inside a .mat file.\n",
      "                        (default: network)\n",
      "  --max-memory-data-size MAX_MEMORY_DATA_SIZE\n",
      "                        Size to start dumping walks to disk, instead of\n",
      "                        keeping them in memory. (default: 1000000000)\n",
      "  --number-walks NUMBER_WALKS\n",
      "                        Number of random walks to start at each node (default:\n",
      "                        10)\n",
      "  --output OUTPUT       Output representation file (default: None)\n",
      "  --representation-size REPRESENTATION_SIZE\n",
      "                        Number of latent dimensions to learn for each node.\n",
      "                        (default: 64)\n",
      "  --seed SEED           Seed for random walk generator. (default: 0)\n",
      "  --undirected UNDIRECTED\n",
      "                        Treat graph as undirected. (default: True)\n",
      "  --vertex-freq-degree  Use vertex degree to estimate the frequency of nodes\n",
      "                        in the random walks. This option is faster than\n",
      "                        calculating the vocabulary. (default: False)\n",
      "  --walk-length WALK_LENGTH\n",
      "                        Length of the random walk started at each node\n",
      "                        (default: 40)\n",
      "  --window-size WINDOW_SIZE\n",
      "                        Window size of skipgram model. (default: 5)\n",
      "  --workers WORKERS     Number of parallel processes. (default: 1)\n"
     ]
    }
   ],
   "source": [
    "!deepwalk --help"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_npz_to_sparse_graph(file_name):\n",
    "    \"\"\"Load a SparseGraph from a Numpy binary file.\n",
    "    from https://github.com/shchur/gnn-benchmark/blob/master/gnnbench/data/io.py\n",
    "    \"\"\"\n",
    "    \n",
    "    from scipy.sparse import csr_matrix\n",
    "    \n",
    "    with np.load(file_name) as loader:\n",
    "        loader = dict(loader)\n",
    "        adj_matrix = csr_matrix((loader['adj_data'], loader['adj_indices'], loader['adj_indptr']),\n",
    "                                   shape=loader['adj_shape'])\n",
    "        attr_matrix = csr_matrix((loader['attr_data'], loader['attr_indices'], loader['attr_indptr']),\n",
    "                                        shape=loader['attr_shape'])\n",
    "        labels = loader['labels']\n",
    "        class_names = loader.get('class_names')\n",
    "\n",
    "    return adj_matrix, attr_matrix, labels, class_names\n",
    "\n",
    "def pca_reduction(X, n_components=2):\n",
    "    pca = PCA(n_components=n_components)\n",
    "    \n",
    "    return pca.fit_transform(X)\n",
    "\n",
    "def kmeans_on_pca_emdedding(embedding_file, y, k=10, pca_n_components=2):\n",
    "    # load resulting embedding\n",
    "\n",
    "    with open(embedding_file, 'r') as f:\n",
    "        # first line is a header containing `number of nodes` and `embeddings size`\n",
    "        n, m = map(int, f.readline().split())\n",
    "        n = 13752\n",
    "        node_embeddings = np.zeros((n, m))\n",
    "        # remaining lines are node_id and embedding vector components\n",
    "        for line in f.readlines():  \n",
    "            try:\n",
    "                node_id, *emb = line.split(' ')\n",
    "                node_embeddings[int(node_id)] = list(map(float, emb))\n",
    "            except:\n",
    "                pass\n",
    "            \n",
    "    # pca\n",
    "    node_embeddings_pca = pca_reduction(node_embeddings, n_components=pca_n_components)\n",
    "    \n",
    "    # k-means      \n",
    "    node_colors_kmeans = KMeans(k).fit_predict(node_embeddings_pca)\n",
    "    order = np.argsort(node_colors_kmeans)\n",
    "    rand_score = adjusted_rand_score(y, node_colors_kmeans)\n",
    "\n",
    "    return node_colors_kmeans, order, rand_score\n",
    "\n",
    "def kmeans_on_embedding(embedding_file, y, k=10):\n",
    "    # load resulting embedding\n",
    "\n",
    "    with open(embedding_file, 'r') as f:\n",
    "        # first line is a header containing `number of nodes` and `embeddings size`\n",
    "        n, m = map(int, f.readline().split())\n",
    "        n = 13752\n",
    "        node_embeddings = np.zeros((n, m))\n",
    "        # remaining lines are node_id and embedding vector components\n",
    "        for line in f.readlines():  \n",
    "            try:\n",
    "                node_id, *emb = line.split(' ')\n",
    "                node_embeddings[int(node_id)] = list(map(float, emb))\n",
    "            except:\n",
    "                pass\n",
    "            \n",
    "    # k-means      \n",
    "    node_colors_kmeans = KMeans(k).fit_predict(node_embeddings)\n",
    "    order = np.argsort(node_colors_kmeans)\n",
    "    rand_score = adjusted_rand_score(y, node_colors_kmeans)\n",
    "\n",
    "    return node_colors_kmeans, order, rand_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "graph_file = 'amazon_electronics_computers.npz'\n",
    "edgelist_file = 'sparce_graph.edgelist'\n",
    "embedding_file32 = 'sparce_graph.embedding32'\n",
    "embedding_file64 = 'sparce_graph.embedding64'\n",
    "embedding_file128 = 'sparce_graph.embedding128'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(13752, 13752)\n"
     ]
    }
   ],
   "source": [
    "adj, X, y, class_names = load_npz_to_sparse_graph(graph_file)\n",
    "print(adj.todense().shape)\n",
    "sparse_g = nx.from_numpy_array(adj)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(edgelist_file, 'w') as f:\n",
    "    for edge in sparse_g.edges:\n",
    "        f.write(f'{edge[0]} {edge[1]}\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### embedding 32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of nodes: 13471\n",
      "Number of walks: 134710\n",
      "Data size (walks*length): 5388400\n",
      "Walking...\n",
      "Training...\n",
      "2023-11-09 17:08:45 WARNING word2vec.py: 1545 Both hierarchical softmax and negative sampling are activated. This is probably a mistake. You should set either 'hs=0' or 'negative=0' to disable one of them. \n"
     ]
    }
   ],
   "source": [
    "!deepwalk --seed 41 --representation-size 32 --format edgelist --input sparce_graph.edgelist --output sparce_graph.embedding32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "adjusted rand score on 32 embedding: 0.409\n",
      "adjusted rand score on 32 embedding with pca reduction: 0.405\n"
     ]
    }
   ],
   "source": [
    "node_colors_kmeans32, order32, rand_score32 = kmeans_on_embedding(\n",
    "                                                                    embedding_file32, \n",
    "                                                                    y, \n",
    "                                                                    k=10\n",
    "                                                                  )\n",
    "\n",
    "node_colors_kmeans32_pca, order32_pca, rand_score32_pca = kmeans_on_pca_emdedding(\n",
    "                                                                                    embedding_file32, \n",
    "                                                                                    y, \n",
    "                                                                                    k=10\n",
    "                                                                                  )\n",
    "\n",
    "print('adjusted rand score on 32 embedding:', np.around(rand_score32, 3))\n",
    "print('adjusted rand score on 32 embedding with pca reduction:', np.around(rand_score32_pca, 3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### embedding 64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of nodes: 13471\n",
      "Number of walks: 134710\n",
      "Data size (walks*length): 5388400\n",
      "Walking...\n",
      "Training...\n",
      "2023-11-09 17:21:29 WARNING word2vec.py: 1545 Both hierarchical softmax and negative sampling are activated. This is probably a mistake. You should set either 'hs=0' or 'negative=0' to disable one of them. \n"
     ]
    }
   ],
   "source": [
    "!deepwalk --seed 41 --representation-size 64 --format edgelist --input sparce_graph.edgelist --output sparce_graph.embedding64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "adjusted rand score on 64 embedding: 0.429\n",
      "adjusted rand score on 64 embedding with pca reduction: 0.382\n"
     ]
    }
   ],
   "source": [
    "node_colors_kmeans64, order64, rand_score64 = kmeans_on_embedding(\n",
    "                                                                    embedding_file64, \n",
    "                                                                    y, \n",
    "                                                                    k=10\n",
    "                                                                  )\n",
    "\n",
    "node_colors_kmeans64_pca, order64_pca, rand_score64_pca = kmeans_on_pca_emdedding(\n",
    "                                                                                    embedding_file64, \n",
    "                                                                                    y, \n",
    "                                                                                    k=10\n",
    "                                                                                  )\n",
    "\n",
    "print('adjusted rand score on 64 embedding:', np.around(rand_score64, 3))\n",
    "print('adjusted rand score on 64 embedding with pca reduction:', np.around(rand_score64_pca, 3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### embedding 128"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of nodes: 13471\n",
      "Number of walks: 134710\n",
      "Data size (walks*length): 5388400\n",
      "Walking...\n",
      "Training...\n",
      "2023-11-09 17:25:44 WARNING word2vec.py: 1545 Both hierarchical softmax and negative sampling are activated. This is probably a mistake. You should set either 'hs=0' or 'negative=0' to disable one of them. \n"
     ]
    }
   ],
   "source": [
    "!deepwalk --seed 41 --representation-size 128 --format edgelist --input sparce_graph.edgelist --output sparce_graph.embedding128"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "adjusted rand score on 128 embedding: 0.429\n",
      "adjusted rand score on 128 embedding with pca reduction: 0.39\n"
     ]
    }
   ],
   "source": [
    "node_colors_kmeans128, order128, rand_score128 = kmeans_on_embedding(\n",
    "                                                                      embedding_file128, \n",
    "                                                                      y, \n",
    "                                                                      k=10\n",
    "                                                                    )\n",
    "\n",
    "node_colors_kmeans128_pca, order128_pca, rand_score128_pca = kmeans_on_pca_emdedding(\n",
    "                                                                                      embedding_file128, \n",
    "                                                                                      y, \n",
    "                                                                                      k=10\n",
    "                                                                                    )\n",
    "\n",
    "print('adjusted rand score on 128 embedding:', np.around(rand_score128, 3))\n",
    "print('adjusted rand score on 128 embedding with pca reduction:', np.around(rand_score128_pca, 3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "score: 0.828\n",
      "adjusted rand score on logreg labels: 0.647\n"
     ]
    }
   ],
   "source": [
    "X_nparr = np.asarray(X.todense())\n",
    "n = int(len(y)*0.15) # разделим выборку на train и test, 85% и 15% соотвественно\n",
    "N = len(y) - n\n",
    "\n",
    "clf = LogisticRegression(random_state=0).fit(X_nparr[:N], y[:N])\n",
    "print('score:', np.around(clf.score(X_nparr[N:], y[N:]), 3))\n",
    "logreg_labels = clf.predict(X_nparr[N:])\n",
    "print('adjusted rand score on logreg labels:', np.around(adjusted_rand_score(y[N:], logreg_labels), 3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### результаты"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Сравнительная таблица\n",
    "\n",
    "| метод | adjusted rand score | \n",
    "| --- | --- |\n",
    "| deep walk emb 32 | 0.409 | \n",
    "| deep walk emb 32 -> 2 | 0.405 | \n",
    "| deep walk emb 64 | 0.428 | \n",
    "| deep walk emb 64 -> 2 | 0.382| \n",
    "| deep walk emb 128 | 0.429 | \n",
    "| deep walk emb 128 -> 2 |0.39 | \n",
    "| log reg | 0.647 | \n",
    "\n",
    "### Вывод\n",
    "\n",
    "анализируя результаты классификации по эмбеддингам deep walk разной размерности, наилучший результат дает кластеризация на эмбеддингах размерности 128, однако результат на эмбеддингах размерности 64 не намного отсает от него. \n",
    "\n",
    "в качестве алгоритма понижения размерности эмбеддингов до размерности 2 использовался PCA. на сжатых эмбеддингах лучший результат получился для 32 -> 2 врианта. Видимо, потому сжимая эмбеддинги большей размерности в 2 не удается сохранить достаточно нужной информации, чтобы сохранить качество кластеризации. \n",
    "\n",
    "наилучший результат классификации дает логистическая регрессия с adjusted rand score 0.647."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
